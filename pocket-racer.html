<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Pocket Racer</title>
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <meta content="" name="keywords">
    <meta content="" name="description">
    <!-- Favicon -->
    <link href="assets/images/favicon.png" rel="icon">
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
    <!-- Stylesheet -->
    <link href="assets/css/bootstrap.min.css" rel="stylesheet">
    <link href="assets/css/style.css" rel="stylesheet">

    





  </head>
  <body>
    <header class="header">
      <div class="container">
        <div class="d-flex justify-content-between align-items-lg-center top-header">
          <div class="logo" data-aos="fade-left">
            <a href="index.html" class="logo-pc"><img src="assets/images/logo.png" alt="logo" width="120"></a>
          </div>
          <nav class="navbar navbar-expand-lg p-0" data-aos="fade-right">
              <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#top-menu" aria-controls="top-menu" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
                <span></span>
              </button>
              <div class="collapse navbar-collapse" id="top-menu">              
                <ul class="navbar-nav">
                  <li class="nav-item">
                    <a class="nav-link" href="index.html">Home</a>
                  </li>
                  <li class="nav-item">
                    <a class="nav-link active" href="pocket-racer.html"> Pocket Racer </a>
                  </li>
                  <li class="nav-item">
                    <a class="nav-link" href="team.html"> Team </a>
                  </li>
                  <li class="nav-item">
                    <a class="nav-link" href="contact.html">Contact</a>
                  </li>
                </ul>
              </div>
            </nav>
          </div>
        </div>
      </header>

      <section class="p-0 banner bg-img">
        <img src="assets/images/bg.jpg" alt="Pocket Racer" data-aos="fade-up">
        <div class="bg-img-content">
            <div class="container">
              <div class="row">
                <div class="col-lg-4 ms-auto">
                  <h1 class="page-heading text-start mb-5 my-lg-3" data-aos="fade-right" style="color: white;">Pocket Racer</h1>
                  <div class="table-content" data-aos="fade-left">
                    <h3 class="heading3 text-start" style="color: white;">Table of Content</h3>
                    <ul class="decimal">
                      <li><a href="#introduction">Introduction</a></li>
                      <li><a href="#background">Background</a></li>
                      <li><a href="#bill-materials">Bill of Materials</a>
                        <ul>
                          <li><a href="#vehicle-components">Vehicle Components</a></li>
                          <!-- <li><a href="#racing-components">Racing Circuit Components</a></li>
                          <li><a href="#diagram-components">Tool Diagram Components</a></li> -->
                        </ul>
                      </li>
                      <!-- <li><a href="#hardware-assembly">Hardware Assembly</a></li>
                      <li><a href="#behavioral-algorithm">Behavioral Conditioning Algorithm</a></li>
                      <li><a href="#github">Github Code</a></li> -->
                      <li><a href="#pocketRacer">Pocket Racer</a></li>
                    </ul>
                  </div>
                </div>
              </div>
            </div>
          </div>
      </section>

      <section id="introduction">
        <div class="container">
          <h3 class="mb-3 text-start heading2" data-aos="fade-right">Introduction</h3>
          <p data-aos="fade-left">One of the most <b>pressing challenges</b> within autonomous mobility lies within highway scenarios: where autonomous vehicles would be interacting with human drivers at high speeds. These scenarios are indeed challenging because autonomous vehicles require data driven methods to identify the intent of opposing vehicles while also simultaneously planning its trajectory. Although full-scale and high-speed testbeds exist (i.e. Roborace), the space and cost requirements are highly demanding. For this reason, we created an open-source, 1:28 scale research platform for high-speed autonomous driving research that is <b>accessible to everyone</b>.</p>
        </div>
      </section>
<style type="text/css">
  .heading3 {
    color: black;
  }
</style>
      <section id="background">
        <div class="container">
          <h3 class="mb-3 text-start heading2" data-aos="fade-left">Background</h3>
          <p data-aos="fade-right">Building a scaled-down autonomous vehicle is quite a complex task. Please keep in mind the following considerations:</p>
          <div class="row mt-4 justify-content-around">
            <div class="col-md-5" data-aos="fade-left">
              <h3 class="heading3">Budget</h3>
              <p>Although the material cost of building a vehicle is less than 100 USD, several tools are needed:</p>
              <ol class="my-4 disc">
                <li>Bsic electronic tools (i.e. screwdriver, soldering tool, glue gun)</li>
                <li>A computer with a GPU (alternative: cloud computing)</li>
                <li>3D printer</li>
              </ol>
              <p>Thus, the <b>real cost</b> of having a fully-functioning vehicle can fluctuate (depending on which tools are readily accessible or not). Please keep this in mind before jumping immediately into the project.</p>
            </div>
            <div class="col-md-5 mt-4 mt-md-0" data-aos="fade-right">
              <h3 class="heading3">Knowledge</h3>
              <p>We provide <b>two ways</b> a vehicle can race itself autonomously. These methods and their resources are articulated below:</p>
              <ol class="my-4 disc">
                <li><b>Deep learning</b>: we recommend viewing <a href="https://youtu.be/5tvmMX8r_OM"> university</a> lectures with topics concerning Deep Learning via YouTube</li>
                <li><b>Sensor Fusion and Probabilistic Robotic Methods</b>: we recommend the <a href="https://www.amazon.com/Probabilistic-Robotics-INTELLIGENT-ROBOTICS-AUTONOMOUS/dp/0262201623"> Probabilistic Robotics (Intelligent Robotics and Autonomous Agents series) 1st Edition book</a> by Sebastian Thrun</li>
              </ol>
              <p>In addition, most of the code that we use is <b>Python</b>. You also need to know how to code within Deep Learning language frameworks. Although the project had began with Keras for Deep Learning, TensorFlow was later used. Alternatives include Pytorch (which has potential merit considering its developments in the area of deployment).</p>
            </div>
          </div>
          <div class="row mt-4 justify-content-around">
            <div class="col-md-5" data-aos="fade-right">
              <h3 class="heading3">3D-Printing</h3>
              <p>It is <b>necessary</b> to have access to a 3D printer. because the parts are (at most) palm-sized, 3D printers around less than 100 USD (such as the Sondori Pico 3D printer) will be enough. We also recommend the Prusa Mini+ or MK3S+ with +0.02mm tolerance PETG filament if you are serious about getting quality prints. Knowing how to use Computer Aided Design (CAD) is also necessary, provided that you are interested in making modifications to your vehicle in the future.</p>
            </div>
            <div class="col-md-5 mt-4 mt-md-0" data-aos="fade-left">
              <h3 class="heading3">Electronics</h3>
              <p>You should be able to understand how to use the following as well:</p>
              <ol class="my-4 disc">
                <li>Raspberry Pi with General Purpose Input Output (GPIO) ports</li>
                <li>hardware pulse width modulation (PWM) signaling and UART protocols</li>
                <li>solder wires and understand how remote controlled cars work</li>
                <li>the basics of First-Person-View (FPV) equipment (if you plan on driving your car with a FPV remote steering wheel setup)</li>
              </ol>
              <p></p>
            </div>
          </div>
        </div>
      </section>

      <section id="bill-materials">
        <div class="container">
          <h2 class="mb-3 text-start heading2" data-aos="fade-right">Bill of Materials</h2>
          <p data-aos="fade-left">Several materials are needed to successfully build a self driving RC car. Our bill of materials will be periodically updated to reflect the latest revisions. Note that the <b>cost of components does not reflect shipping cost</b>. In addition, there are a total of <b>three ways</b> of controlling the vehicle for data collection. The <b>recommended method</b> is the low latency option â€“ as this offers the quick communication protocols.</p>
          <div id="vehicle-components" class="mt-5">
            <h3 class="heading3" data-aos="fade-down">Vehicle Components</h3>
            <img src="assets/images/home-img4.png" class="mx-auto d-block" alt="Vehicle Components" data-aos="fade-up">
          </div>
          <div class="d-flex flex-wrap mt-4 justify-content-center">
            <div class="vc-block" data-aos="fade-right">
              <h4>Chassis</h4>
              <ul class="decimal">
                <li>Kyosho Mini-Z MR-03 1:28 RTR Kit</li>
                <li>PN Racing 2.5W Chassis Conversion Kit</li>
                <li>PN Racing v3 Micro Servo Kit</li>
              </ul>
            </div>
            <div class="vc-block" data-aos="fade-left">
              <h4>Sensors</h4>
              <ul class="decimal">
                <li>US 1881 Latch Type Hall Sensor [x4]</li>
                <li>Neodymium Magnet [x16]</li>
                <li>Digital Battery Voltage Meter</li>
                <li>Battery Current Meter</li>
              </ul>
            </div>
            <div class="vc-block" data-aos="fade-right">
              <h4>Compute Unit</h4>
              <ul class="decimal">
                <li>Raspberry Pi 4B 4G</li>
                <li>Samsung Evo Plus 256G SD Card</li>
                <li>Risun RFB2008H 5v Blower Fan</li>
                <li>OSKJ 5V-3A DC Step-Down Buck Converter</li>
              </ul>
            </div>
            <div class="vc-block" data-aos="fade-right">
              <h4>Batteries and Shell</h4>
              <ul class="decimal">
                <li>Vapecell 3.7v 1S 350mAh Li-Ion Battery [x4]</li>
                <li>Mini-Z Truck Lexan Body Shell</li>
                <li>Wheel Encoder Ring [x4]</li>
                <li>Tamiya PS Spray Paint</li>
              </ul>
            </div>
            <div class="vc-block" data-aos="fade-left">
              <h4>3D-Printed Parts</h4>
              <ul class="decimal">
                <li>Camera Mount</li>
                <li>RPi Board Mount</li>
                <li>Wheel Encoder Ring [x4]</li>
                <li>Rear Mount T-Plate</li>
                <li>Mount Standoffs</li>
              </ul>
            </div>
          </div>
<!--           <div id="racing-components" class="mt-5">
            <h3 class="heading3" data-aos="fade-down">Racing Circuit Components</h3>
            <img src="assets/images/racing-circuit-diagram.png" class="mx-auto d-block" alt="Racing Circuit Components" data-aos="fade-up">
          </div>
          <div class="d-flex flex-wrap mt-4 justify-content-center">
            <div class="vc-block" data-aos="fade-right">
              <h4>First Person View</h4>
              <ul class="decimal">
                <li>Fatshark Scout FPV Goggles</li>
                <li>TBS Nano Pro32 Video FPV Rx (27)</li>
                <li>RunCam Eagle 3 FPV Camera (26)</li>
              </ul>
            </div>
            <div class="vc-block" data-aos="fade-left">
              <h4>Controller</h4>
              <ul class="decimal">
                <li>Futaba T6J 6Ch. 2.4Ghz Tx Controller with Trainer Mode</li>
                <li>(Driving Mode Option) Thrustmaster 500RS / 300RS Gaming Wheel</li>
                <li>Compufly v2.0 PPM2USB Futaba Ver.</li>
                <li>(Budget Option) Logitech F710 Gaming Pad Controller</li>
                <li>(Low Latency Option) Sanwa M17 RC Controller + RX 493 Receiver</li>
              </ul>
            </div>
            <div class="vc-block" data-aos="fade-right">
              <h4>Racing Circuit</h4>
              <ul class="decimal">
                <li>RCP Tracks Mini-96 Basic Set</li>
                <li>RCP Tracks Mini-96 Expansion Tiles</li>
                <li>Orange Mini Soccer Cones</li>
                <li>Orange Lap RC 4ch. PWM2USB Dongle (23)</li>
              </ul>
            </div>
          </div>
          <div id="diagram-components" class="mt-5">
            <h3 class="heading3" data-aos="fade-down">Tool Components</h3>
            <div class="row">
              <div class="col-md-6 mb-4 mb-md-0" data-aos="fade-right">
                <p class="fs-5 text-center mb-3"><b>Measurement Tools</b></p>
                <img src="assets/images/tools.png" class="mx-auto d-block" alt="tools">
                <ul class="decimal mt-3">
                  <li>Tamiya Speed Checker</li>
                  <li>SkyRC Corner Weight Scale</li>
                  <li>ARR Aurora Car Setup System</li>
                </ul>
              </div>
              <div class="col-md-6" data-aos="fade-left">
                <p class="fs-5 text-center mb-3"><b>Global Tracking Tool</b></p>
                <img src="assets/images/overhead-cam.png" class="mx-auto d-block" alt="tools">
                <ul class="decimal mt-3">
                  <li>Logitech Brio 4k Web Camera</li>
                  <li>Theia Wide Angle CS Lens</li>
                  <li>Kurokesu Brio CS Mount Kit</li>
                  <li>USB 3.0 20ft. Extension Cable</li>
                </ul>
              </div>
            </div>
          </div>
          <p class="fs-5 my-4" data-aos="fade-up">With our vehicle and track components <b>gathered</b>, we can now <b>assemble</b> our fully autonomous radio control car.</p> -->
        </div>
      </section>

<!--       <section id="hardware-assembly">
        <div class="container">
          <h2 class="mb-3 text-start heading2" data-aos="fade-up">Hardware Assembly</h2>
          <div class="row align-items-center justify-content-between">
            <div class="col-md-6" data-aos="fade-left">
              <p class="fs-5">As supplemental content, we provide an overhead diagram for wiring the vehicle electronic components together. To <b>avoid inhaling soldering fumes</b>, it is highly recommended that a mini desk fan is bought. It is also <b>recommend</b> to buy multiple soldering tips (considering the fact that they wear out). Lastly, it is recommend that soldering paste is used (however, make sure to minimize the amount of paste to prevent the wires from shorting).</p>
            </div>
            <div class="col-md-5" data-aos="fade-right">
              <img src="assets/images/home-img5b.png" alt="Hardware Assembly">
            </div>
          </div>
        </div>
      </section> -->

<!--       <section id="behavioral-algorithm">
        <div class="container">
          <h2 class="mb-3 text-start heading2" data-aos="fade-down">Behaviorial Conditioning Algorithm</h2>
          <div class="row align-items-center justify-content-between">
            <div class="col-md-6" data-aos="fade-left">
              <p>Using our proposed testbed, we propose <b>Behavioral Conditioning</b> (BCo): an improved end-to-end controller specifically designed for high-speed autonomous driving. An LSTM-VAE neural network creates an approximator policy that predicts the desired steering action and motor-speed given an image input. A PID controller is also used to track the desired motor-speed with the observed target motor-speed using throttle action. Our algorithm is specifically designed to address the issue of noisy action outputs (of which is prevalent when directly correlating image inputs to action outputs).</p>
            </div>
            <div class="col-md-5" data-aos="fade-right">
              <img src="assets/images/home-img7.png" alt="Behaviorial Conditioning Algorithm">
            </div>
          </div>
          <div class="row align-items-center justify-content-between">
            <div class="col-md-6" data-aos="fade-right">
              <img src="assets/images/3d-cnn-model.png" alt="Behaviorial Conditioning Algorithm">
              <p>First, BCo decouples both the task of low level vehicle control and high level path planning to address noise in throttle action outputs. While existing work attempts to solve the issue of noisy inference output via hardware, a hardware constrained scenario within this open-source platform makes it difficult to achieve. By decoupling the task of high speed autonomous driving using a PID controller, it is possible to directly track desired motor shaft speed as to <b>alleviate</b> any noisy throttle commands when inferencing action commands directly with convolutional neural networks.</p>
            </div>
            <div class="col-md-5" data-aos="fade-left">
              <p>Second, BCo <b>addresses</b> noisy outputs by using a Variational Auto-Encoder (VAE) architecture to make the neural network output continuous. Continuous action outputs are achieved by using the KL-divergence equation as shown to regularize the latent variables. The latent variables (which consist of gaussians) use the KL-divergence as a regularization term such that their distributions closely match a standard normal distribution â€“ thus encouraging continuity between action outputs when given similar image inputs.</p>
            </div>
          </div>
        </div>
      </section> -->

<!--       <section id="github">
        <div class="container">
          <h2 class="mb-3 text-start heading2" data-aos="fade-right">Github Code</h2>
          <p data-aos="fade-left">Our <a href="https://github.com/jessecha/MACHInE"> Github</a> repository contains all the necessary code needed for the open-source racecar project. The purpose of each folder and what it contains is also documented below.</p>
          <div class="row mt-4">
            <div class="col-md-4 col-lg-3 mb-4" data-aos="fade-right">
              <h3 class="heading3">1. Data Collection</h3>
              <p>The â€˜data_collectionâ€™ folder includes six folders for the Robot Operating System (ROS). It also contains the launch file for data collection. One can selectively record specific ROS topics after launching the launch file using the â€˜rosbag recordâ€˜ command. During data collection, it is important to use the <b>isocpus command</b> to isolate CPU core: 0 within the Raspberry Pi (as to maximize the number of collected frames).</p>
            </div>
            <div class="col-md-4 col-lg-3 mb-4" data-aos="fade-left">
              <h3 class="heading3">2. Train</h3>
              <p>The â€˜trainâ€˜ folder contains the â€˜dataparserâ€˜ folder to convert rosbag recordings into a csv file format. The â€˜lrcn_motor.pyâ€˜ script is used to <b>train</b> the model. Tensorflow 2.4 was used with Tensorflow lite to convert and quantize the model. There are several models to choose from in the model folder (however, it's worth noting that that the vision transformer model might not be supported with tflite during conversion).</p>
            </div>
            <div class="col-md-4 col-lg-3 mb-4" data-aos="fade-right">
              <h3 class="heading3">3. Deploy</h3>
              <p>The â€˜deployâ€˜ folder contains a script to deploy the tflite model.</p>
            </div>
            <div class="col-md-4 col-lg-3 mb-4" data-aos="fade-left">
              <h3 class="heading3">4. Evaluate</h3>
              <p>The â€˜evaluateâ€˜ folder contains the following: a ROS folder to record the logitech brio stream and a script called, the â€˜ground_truth_tracker.ipynbâ€˜ which processes the rosbag into a video, and scripts for salient feature representation.</p>
            </div>
            <div class="col-md-4 col-lg-3 mb-4" data-aos="fade-left">
              <h3 class="heading3">5. Tools</h3>
              <p>The â€˜toolsâ€˜ folder contains <b>useful</b> commands. The usb_list.sh command lists all the devices linked to the Raspberry Pi board / desktop via USB port.</p>
            </div>
            <div class="col-md-4 col-lg-3 mb-4" data-aos="fade-right">
              <h3 class="heading3">6. STL Files</h3>
              <p>This folder contains all the necessary STL files needed to <b>print</b> the vehicle mounts. It is important to note that all the Ethernet ports and USB ports must be <b>stripped</b> prior to assembly. A quality print is necessary for the â€˜t_connector.stlâ€˜ file as an uneven print will cause the vehicle to warp, resulting in unwanted behavior when trying to drive straight</p>
            </div>
          </div>
          <p class="fs-5 my-4" data-aos="fade-left">As a result of reading up until this point, hopefully you now have an adequate understanding in building your own fully autonomous scaled-down radio control car. Of course, we <b>encourage</b> you to connect with racers and have fun! If you have any questions, feel free to contact us.</p>
        </div>
      </section> -->
      <section class="brdr" id="pocketRacer">
        <div class="container">
          <h2 class="text-center m-4" data-aos="fade-down">Pocket Racer: An Accessible Platform for Multi-Agent Autonomous Racing</h2>
          <div class="row">
            <div class="col-md-6" data-aos="fade-left">
              <p><strong>
                <i>Abstract</i>â€”Scaled autonomous racing platforms are popular within Cyber Physical Systems (CPS) research and education, where Ackerman-steering robots of varying sizes compete head- 2-head at high speeds. These robots are required to drive at the limit of handling while safely overtaking other agents within its environment. However, existing platforms are inaccessible due to their large cost and/or size requirements to enable such high speeds. Addressing these issues, we present Pocket Racer, a low-cost, miniature, autonomous racing platform capable of racing within a typical university hallway or classroom setting. Pocket Racer uses Vision Transformers tailored for autonomous racing (RaceViT) while Learning from Demonstrations (LfD) shared across multiple vehicles. We open-source Pocket Racer with a dedicated website detailing build instructions and code in hopes of making autonomous racing accessible. <br>
                <i class="ms-3">Index Terms</i>â€”Cyber Physical Systems, Imitation Learning, Autonomous Driving
              </strong></p>
            </div>
            <div class="col-md-6" data-aos="fade-right">
              <img src="assets/images/pocket-racer-img1.png" alt="pocket racer">
              <p class="mb-4">Fig. 1. <b><i>Pocket Racer</i></b>: An Accessible Platform for Multi-Agent Autonomous Racing</p>
            </div>
            <div class="col-md-6" data-aos="fade-right">
              <p class="text-center"><b>I. INTRODUCTION</b></p>
              <p>Autonomous racing is an active area of research in Cyber Physical Systems (CPS), currently creating milestone events in the intersection of artificial intelligence and robotics. These high speed platforms are also popularized as educational tools for undergraduate education. Given sufficient space, these platforms can autonomously perform multi-agent maneuvers such as overtaking. While full scale autonomous platforms (A2RL, Indy Autonomous Challenge, Roborace ) are the defacto options for racing, they require industry-scale involvement to meet their extremely large testing facilities and space requirements. As such, this option is inaccessible for most research institutions. Another commonly used so- lution is within virtual simulation. Open-source solutions in simulation exist (CARLA), but often do not offer the dynamics of a physical platform.</p>
              <p>Due to its accessibility, many scaled autonomous platforms</p>
            </div>
            <div class="col-md-6" data-aos="fade-left">are open-source as an alternative within autonomous racing. Unfortunately, they still impose significant space requirements due to their large size. Furthermore, they typically utilize costly LIDAR sensors for autonomy, making it expensive to maintain a fleet of vehicles. While these requirements are achievable for the purposes of single-agent racing, it poses a significant challenge for multi-agent racing. As a result, miniature, low-cost autonomous vehicles (1/43th, 1/28th scale) have been proposed specifically for indoor testing in confined spaces. However, these platforms has been one of compromise, with vehicles going smaller at the expense of fidelity or autonomy. Some examples include the offloading of computation/sensors needed for localization or a compromise of vehicle dynamics; i.e. suspension geometry or a differential drive.</div>
          </div>
        </div>
      </section>
      <hr>
      <section class="brdr">
        <div class="container">
          <div class="row">
            <div class="col-md-6" data-aos="fade-right">
              <p>Thus, existing literature indicates that a miniature, high fidelity, autonomous robotic platform may be useful for research and education. As a solution, we propose Pocket Racer, a low-cost, miniature platform for multi-agent au- tonomous racing. The vehicles are capable of autonomy with a single monocular camera and a Raspberry Pi Zero 2, accessibly allowing high speed maneuvers in constrained environments. The vehicles can operate autonomously at the limit of handling with speeds upwards of 20 km/h. The vehicles are of low-cost, using only 3D printed parts with off the shelf electronics to achieve a cost of less than 250 USD for a single vehicle. These accessible features enable an analysis of autonomous racing in a typical indoor hallway or classroom.</p>
              <p>We further demonstrate the capabilities of our platform by utilizing vision transformers (ViTs) designed for efficient high speed autonomy on the edge, which we call RaceViT. Our RaceViT algorithm captures the sequential progression of the environment scene by use of horizontal lines as tokens to the transformer. However, we further improve upon this approach using two methods. First, we enable fast inference of actions (i.e. steering) with a quantized Tensorflow lite model which allow the vehicle to move at higher speeds. Second, our model is trained by pooling collected demonstrations across multiple platforms, allowing efficiency during data collection. Our platform validates our proposed algorithm against baseline models for not just accuracy, but also deployment through empirical results.</p>
              <p>The contributions of our work is the following:</p>
              <ol>
                <li><i>he design of Pocket Racer, an open-source, low-cost, miniature platform for multi-agent autonomous racing.</i></li>
                <li><i>Development of Vision Transformers specific for high speed autonomous driving (RaceViT).</i></li>
                <li><i>Performance evaluations of our platform and algorithm design for a diverse set of indoor environments.</i></li>
                <li><i>An open-source repository and dedicated website for Pocket Racer.</i></li>
              </ol>
              <p class="text-center mt-4 mb-2"><strong>II. RELATED WORK</strong></p>
              <p>In recent years, the improved computational capabilities of embedded systems enabled the growth of scaled autonomous vehicle platforms via the retrofitting of hobbyist remote control (RC) racing vehicles. Many scaled autonomous vehicles offer high-fidelity testing for speeds at the limit of handling (1/5th, 1/8th). Their size however, makes it only suitable for outdoor environments. Slightly smaller indoor platforms (1/10th, 1/18th) exist for hobbyists and high-school level educational outreach, as well as university level research, but due to its large size, is not suitable for multi-vehicle testing in a typical classroom environment.</p>
              <p>Head-2-Head racing is a growing trend within autonomous racing, but as most platforms utilize expensive 2D LIDAR sensors on large 1/10th scale platforms, they are inaccessible for classroom use. Recently, there has been growing participation from the industry (DJI Robomaster, Nvidia JetRacer) to push for scaled platforms as a educational platform for autonomous racing.</p>
            </div>
            <div class="col-md-6" data-aos="fade-left"> 
              <p>However, these platforms often lack the vehicle specifications (offroad wheels, high-center of gravity, ) needed for multi-agent driving and/or are mostly designed to work with proprietary software.</p>
              <p>DonkeyCar and Amazon AWS DeepRacer are examples of smaller platforms for autonomous racing. The communities have an active forum and extensive documentation on their platform build. However, their platform size (1/16th) still remains an issue for typical classroom use, suggesting an need to develop palm-sized platforms to further enable research in this area.</p>
              <p>Some low-cost, minature platforms for indoor testbeds have been developed for fleet driving, but typically lack the Ackerman-steering configuration necessary for high speed analysis. Other miniature vehicle platforms with Ackerman-steering have been used for high speed testing, but do not have onboard autonomy. While it is true that some of the platforms themselves are low-cost, the platforms require expensive motion capture systems for localization, making them inaccessible. Thus, it is clear that there is a need to develop an accessible autonomous platform that can enable high-speed multi-agent autonomous racing in indoor environments.</p>
              <img src="assets/images/pocket-racer-img2.png" alt="pocket racer">
              <p class="text-center mt-4 mb-2"><strong>III. METHODOLOGY</strong></p>
              <p><b><i>A. Algorithm</i></b></p>
              <p>Our algorithm, RaceViT, utilizes a variant of the Vision Transformer (ViT), with several improvements specific towards autonomous racing. First and foremost, the image tokens fed to the attention encoder network are processed horizontally in lieu of patches as a stack. By use of horizontal patches from the bottom of the image, the transformer is able to process images in the sequence at which the vehicle pro- gresses forward, making full use of the positional embedding found in the attention encoder. A further benefit to horizontal tokenizing of images is that it preserves spatial information otherwise lost when flattening patches of images originally</p>
            </div>
          </div>
        </div>
      </section>
      <hr>
      <section class="brdr">
        <div class="container">
          <div class="row">
            <div class="col-md-6" data-aos="fade-left">
              <img src="assets/images/pocket-racer-img3.png" alt="pocket racer">
              <p class="mb-4">used in ViTs. Other improvements include the quantization of our model via Tensorflow lite and the pooling of datasets created by multiple vehicles during data collection.</p>
              <p><b><i>B. Testbed Design</i></b></p>
              <p class="mb-4"> concerning issue with many indoor robotic testbeds is that they rely on motion capture systems as a means of providing vehicle state information, i.e. localization. A typical motion capture system (Vicon, Optitrack) is too expensive for many laboratories with a cost going upwards of 20K USD for a complete rig. While these systems are necessary for applications such as drones requiring tracking in space, it is not necessary for wheeled autonomous racing. As a result, a commercial webcam is retrofitted with a low distortion (1%) CS-mount lens to function as a overhead tracker, capable of tracking the vehicle at 90 FPS (in bright conditions) with an approximate error of less than 5cm for a 11 x 16 ft. circuit. The tracker when positioned on a ceiling can cover a typical indoor circuit with its wide 130 degrees field of view. It uses a sequence of frames to estimate the vehicle velocity and position using color based background subtraction. Finally, while Pocket Racer is designed for accessible use in classrooms with the foam tile circuit, it is also capable of autonomous racing within carpet hallways found in universities as shown in Table 2.</p>
              <p><b><i>C. Chassis Design</i></b></p>
              <p>The objective was to develop an low-cost and miniature platform for multi-agent autonomous racing. We utilized a single monocular camera paired with a Raspbery Pi Zero 2 W compute module to enable low-cost autonomy. Furthermore, the vehicle was converted from a AWD drivetrain to a RWD drivetrain by simply removing the center gear shaft to reduce inefficiencies due to gear friction, thereby allowing for longer run times. The vehicle also utilized taped radial tires for both the front and rear to maximize traction performance on the carpet surface. The chassis components housing the sensors were fully printable in less than 12 hours with a solid infill.</p>
              <p>Prioritizing high speed performance, a high-torque brushless DC motor (3500Kv) was paired with a low gear ratio differential to provide the torque needed to maximize throttle response. The camera was mounted at the front as high as possible to maximize the field of view necessary for autonomy. This resulted in an accessible yet, high performance</p>
            </div>
            <div class="col-md-6" data-aos="fade-right">
              <p>vehicle with top speeds upwards of 26 km/h. We designed the platform to be accessible only using off the shelf components with the exception of a few 3D printable parts. These custom chassis components (i.e. body shell and battery mount) were designed to be fully printable in less than 12 hours.</p>
              <img src="assets/images/pocket-racer-img4.png" alt="pocket racer" class="d-block mx-auto mt-3">
              <p class="text-center mt-4 mb-2"><strong>IV. EXPERIMENTS & RESULTS (SKIP READING:TO-DO)</strong></p>
              <img src="assets/images/pocket-racer-img5.png" alt="pocket racer" class="d-block mx-auto">
            </div>
          </div>
        </div>
      </section>
      <hr>
      <section class="brdr">
        <div class="container">
          <div class="row">
            <div class="col-md-6" data-aos="fade-right">
              <p><i>A. Data Collection</i></p>
              <p><i>B. Training</i></p>
              <p>&nbsp;&nbsp; The table below tabulates the results of our training experiments.</p>
              <p><i>C. Deployment</i></p>
              <p>&nbsp;&nbsp; We conduct several comparison experiments of two high speed vehicle maneuvers to validate and test our proposed RaceViT algorithm. We compare our RaceVit against other behavioral cloning algorithms by testing maneuvers at different speeds and evaluating the frequency of collisions.</p>
              <p><i>1) Lane Keeping:</i> We compare both our proposed Race-ViT against other working benchmarks for a single vehicle scenario involving lane keeping.</p>
              <p><i>2) Overtaking:</i> We demonstrate a scenario where a autonomous vehicle (#1) overtakes a slower second autonomous vehicle (#2) on both corners and straights.</p>
              <p class="text-center mt-4 mb-2"><strong>V. DISCUSSION</strong></p>
              <p><b><i>A. Hardware Build</i></b></p>
              <p>&nbsp;&nbsp; Initially, the brushed motor and integrated electronic speed controller (ESC) / receiver (RX) from the Ready-to-Run (RTR) chassis was utilized for the power train by inter cepting PWM signals from the Pulse Width Modulation (PWM) LED Driver, as this would lower costs associated with a brushless motor setup and a separate RC controller. However, the brushed motor proved to be problematic due to overheating over time, resulting in inconsistencies in vehicle speed despite a constant application of throttle. The receiver on-board the chassis also proved to be noisy for data collection, often spiking in values. Therefore, we replaced the RC controller with an off-the-shelf Bluetooth gamepad and utilized a brushless motor with a dedicated ESC.</p>
              <p class="mt-3"><b><i>B. Data Collection</i></b></p>
              <p>&nbsp;&nbsp; A sufficient amount of data (200k images) was needed to ensure that the vehicles did not exhibit drift at a given position during test time. The amount of data needed de- pended on the complexity of the circuit as well as the rate at which the data was collected. For a simple oval circuit, a vehicle showed the capability to operate at 10 km/h without collisions with data collected for 15 minutes at 80hz, roughly the time it took to deplete one charge of the vehicle batteries. For large closed-loop hallways, at least 30-40 minutes of human demonstrations were needed, with a minimum of 20 demonstrations of overtaking maneuvers.</p>
              <p>&nbsp;&nbsp; Prior work collected separate trajectories that demonstrated a return-to-lane action sequence in a lane departure event to solve the problem of covariate shift. However, our results showed that a data set size of 200k images proved suf- ficient to work in a closed-loop hallway environment, without any explicit lane markings or center lanes. A limitation with the data collection method was that the vehicles needed to be chased and driven by humans running after the vehicle, which proved to be labor intensive. </p>
            </div>
            <div class="col-md-6" data-aos="fade-left">
              <p>&nbsp;&nbsp; This resulted in the data collection phase consisting of only steering commands with a preset speed to offload the difficulty in also controlling throttle commands. The dataset also needed to be collected in consistent lighting during night time, which proved to be difficult due to transparent doors and changing lighting conditions. Finally, the dataset needed to be pruned for collisions and any external obstacles (i.e. humans) which altered the images.</p>
              <p class="mt-3"><b><i>C. Model Training</i></b></p>
              <p>&nbsp;&nbsp; An image of 32 x 32 RGB pixels was tokenized into 32 horizontal layers as input to the RaceViT. The RaceViT consisted of four attention heads per layer for a total of 2 layers. Finally, the transformer layer was followed by two dense layers of 128 to output steering values for vehicle autonomy. Results showed that our proposed RaceViT architecture had the highest R-squared accuracy for both of the evaluated scenarios, when using two stacked images. Experimental tests showed that cropping the environment had a detrimental impact on the accuracy as the model utilized the entire scene to achieve autonomy.</p>
              <p>&nbsp;&nbsp; An important observation we found was that the model required a large batch size to drive smoothly and generalize autonomous driving behavior. Smaller batch sizes improved accuracy at a cost of over-fitting the dataset such that the vehicle would exhibit highly noisy steering behavior, destabilizing the model. The batch size was maximized at 8000 images to fit the GPU memory limit of 11GB on a Nvidia RTX2080Ti. Within less than an hour of training, the model was able to drive at high speeds without collisions and exhibit overtaking maneuvers up to 21km/h on the carpet hallways within Pepperdine University.</p>
              <p class="mt-3"><b><i>D. Deployment</i></b></p>
              <p>&nbsp;&nbsp; Model inference speed enabled with a high frame per second (FPS) mode on the Raspberry Pi v3 Camera was critical in insuring the vehicleâ€™s ability to overtake and successfully exit corners at high speeds without collisions. The Raspberry Pi V3 camera showed a varying 80-90 FPS when streaming images at a size of 32 x 32 RGB pixels. While an image size of 80 x 80 proved to be optimal in terms of R squared accuracy, a lower pixel count allowed the RaceViT model to inference faster. Therefore, experiments were conducted to determine the image size that would allow for robust high speed autonomy without sacrificing accuracy. Results showed that beyond 32 x 32 pixels, the image proved to be too small to contain the necessary state information of its environment to drive reliably, resulting in collisions despite a sufficient dataset size.</p>
              <p>Other deployment considerations involved the stacking of images. While stacking multiple images as input to the RaceViT model increased model performance as well as smoothed the steering actions, this proved to be computationally expensive during deployment as the delay in corner entry increased linearly with respect to the frame count. The model showed a certain capacity to be resilient to an increase in throttle speed, but resulted in wider turns, creating oscillatory behavior that would often destabilize the vehicle into a collision.</p>
            </div>
          </div>
        </div>
      </section>
      <hr>
      <section class="brdr">
        <div class="container">
          <div class="row">
            <div class="col-md-6" data-aos="fade-left">
              <p>Results showed that operating the vehicle with a speed identical to the speed at which the human demonstrations were collected proved to be most successful in reducing the number of collisions for a continuous series of autonomous laps. Tests also showed that there was no performance loss with converting RGB to grayscale, suggesting the use of a monochrome camera for future work. Finally, model quantization using the tensorflow t flite op timization AP I reduced the model size to a quarter and increased the inference rate with no noticeable decrease in performance.</p>
              <p class="text-center mt-4 mb-2"><strong>VII. ACKNOWLEDGEMENTS</strong></p>
              <p>This work was supported in part by the Keck Institute for Data Science at Pepperdine University. Special thanks to Kevin Hong for his help with the scientific illustrations. We also thank Dr. Fabien Scalzo from the Department of Computer Science for technical advice.</p>
            </div>
            <div class="col-md-6" data-aos="fade-right">
              <p class="text-center mt-4 mb-2"><strong>VI. CONCLUSION</strong></p>
              <p>In conclusion, we present Pocket Racer, a multi-agent racing platform made accessible for research and education. We test and validate our proposed RaceViT algorithm for Pocket Racer, enabling efficient on-board autonomy. Our experimental results were largely successful in exhibiting high speed overtaking maneuvers by learning from demonstrations (LfD). The entire hardware components are 3D printable and all of the electronics are off-the-shelf, allowing for ease of access in educational outreach for STEM fields. Furthermore, we open source Pocket Racer with our project website and code repository. Future work may explore alternative methods of on-device reinforcement learning using neuro-morphic computing. Through our work, we hope to further robotic racing for accessible research and education.</p>
            </div>
          </div>
        </div>
      </section>



      <footer class="mt-4">
        <div class="container text-center">
          <p class="mb-2">Natural Sciences Division. Pepperdine University. Malibu, CA, USA, 90263</p>
          <p>&copy; All right Reserved</p>
        </div>
      </footer>


    <!-- Back to Top -->
    <button type="button" class="back-to-top"></button>
    <!-- JavaScript Libraries -->
    <script src="assets/js/jquery-3.6.0.min.js"></script>
    <script src="assets/js/bootstrap.bundle.min.js"></script>
    <script src="assets/js/main.js"></script>
  </body>
</html>


